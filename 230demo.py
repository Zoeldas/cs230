# -*- coding: utf-8 -*-
"""230demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LvD8nl0D6UXzPpNuu7M_5dNGSHmKwAmV
"""

!pip install ultralytics

import cv2
import torch
import numpy as np
from PIL import Image
from torchvision import transforms
from transformers import Blip2Processor, Blip2ForConditionalGeneration

# Example libraries/models (replace with your favorite ones)
# ----------------------------------------------------------
#   - ultralytics/yolov5 for object detection
#   - huggingface transformers for image captioning or VQA
# ----------------------------------------------------------

# 1. Load your models
# -------------------
#   a. YOLO (for object detection): You can install via "pip install ultralytics"
#   b. A Vision-Language model (like BLIP, BLIP2, or another captioning model)

# Object Detection Model: YOLO
# ----------------------------------------------------------
try:
    from ultralytics import YOLO
    yolomodel = YOLO("yolov5s.pt")  # or your custom-trained weights
except ImportError:
    yolomodel = None
    print("YOLO model not found. Please install ultralytics for YOLO support.")

# Image Captioning Model (e.g., BLIP from Hugging Face)
# -----------------------------------------------------
from transformers import BlipProcessor, BlipForConditionalGeneration

# processor = BlipProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
# caption_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b")

processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
caption_model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16)


def analyze_image(image_path: str) -> dict:
    """
    Analyze a single image with:
    1. Object detection
    2. Image caption generation
    3. (Optional) Additional logic to guess the action or context
    Returns a dictionary of results.
    """

    # 2. Load image
    # -------------
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f"Could not read image at path: {image_path}")

    # 3. Object Detection (if YOLO is available)
    # ------------------------------------------
    objects = []
    if yolomodel is not None:
        # YOLO expects images in RGB as a NumPy array or path
        results = yolomodel.predict(source=image, conf=0.25, show=False)
        # results is typically a list of 'Boxes' with xyxy coords, labels, confidence
        if len(results) > 0:
            # Take the first result if there's only one batch
            detection = results[0].boxes
            for box in detection:
                # Convert box to dictionary
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                label_id = int(box.cls[0])
                confidence = float(box.conf[0])
                # YOLO model has built-in class names
                label_name = yolomodel.names[label_id] if hasattr(yolomodel, 'names') else str(label_id)
                objects.append({
                    "label": label_name,
                    "confidence": confidence,
                    "bbox": [x1, y1, x2, y2]
                })

    # 4. Generate a caption using a Vision-Language model
    # ---------------------------------------------------
    # Convert OpenCV's BGR to RGB, because PIL & BLIP expect RGB
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    pil_image = Image.fromarray(image_rgb)
    inputs = processor(images=pil_image, return_tensors="pt")
    captions = caption_model.generate(**inputs)
    caption_text = processor.decode(captions[0], skip_special_tokens=True)

    # 5. (Optional) Additional “Action Context” or “Scene Reasoning”
    # --------------------------------------------------------------
    # For more advanced “action recognition,” you might use a specific model
    # that classifies what's happening (e.g., someone crawling, handing over an object).
    # Here we’ll keep it simple with a straightforward guess:
    # If objects contain "person" and the caption references "running" or "race",
    # you might guess it's a running competition, etc.

    inferred_action = "Not determined"
    # if any(obj['label'] in ["person"] for obj in objects):
    #     # Simple heuristic:
    #     if "running" in caption_text or "race" in caption_text:
    #         inferred_action = "Likely a runner in a race"
    #     elif "crawling" in caption_text or "falling" in caption_text:
    #         inferred_action = "A person crawling or falling"
    #     # You could improve this logic with an actual action-recognition model

    # 6. Return the combined analysis
    # -------------------------------
    return {
        "caption": caption_text,
        "objects_detected": objects,
        "inferred_action": inferred_action
    }

# 7. Main pipeline to process multiple images
# -------------------------------------------
def main():
    image_paths = [
        "car1.jpg",
        "car2.jpg",
        "car3.jpg",
    ]

    overall_description = []

    for img_path in image_paths:
        analysis_result = analyze_image(img_path)
        overall_description.append(analysis_result)
        print(f"Analysis for {img_path}: {analysis_result}")

    # 8. Summarize across all images
    # ------------------------------
    # This step might involve combining the captions/object detections from each image,
    # then passing them to a language model to get a single summary.
    # We’ll keep it simple and just print them.
    for idx, result in enumerate(overall_description):
        print(f"Image {idx+1} Analysis:")
        print("  - Caption: ", result["caption"])
        print("  - Objects Detected: ", result["objects_detected"])
        print("  - Inferred Action: ", result["inferred_action"])
        print()

    # (Optional) Additional logic to guess the overall event or story
    # For example, if multiple images mention "runner," "crawling," "exhaustion,"
    # you might guess "An exhausted runner is crawling to the finish line in a relay race."

if __name__ == "__main__":
    main()